---
title: "评估工程正成为下一轮 Agent 演进的重点"
description: "评估工程正成为下一轮 Agent 演进的重点"
date: "2025-10-30"
category: "article"
keywords: ["Higress"]
authors: "CH3CHO"
---

作者：望宸&马云雷



阿里云 CIO 蒋林泉曾分享过：在落地大模型技术过程中总结过一套方法论，叫 RIDE，即 Reorganize（重组组织与生产关系）、Identify（识别业务痛点与 AI 机会）、Define（定义指标与运营体系）、和 Execute（推进数据建设与工程落地）。其中，Execute 提到了评估系统重要性的核心原因，即这一轮大模型最关键的区别在于：度量数据和评测均没有标准的范式。这就意味着，这既是提升产品力的难点，同时也是产品竞争力的护城河。



在 AI 领域里经常提到一个词叫“品味”，这里讲的“品味”，其实就是如何设计评估工程，对 Agent 的输出进行评价。



## 一、从确定性到不确定性
在传统的软件研发中，测试是开发过程中必不可少的一环，测试保障了输入和输出的确定性，以及向前兼容性，是软件质量保障的基础。测试覆盖率和准确率是评价质量的指标，而准确率必须保持在100%的水平。

+ 传统软件在很大程度上是确定性的。给定相同的输入，系统将始终产生相同的输出。其故障模式，即“bug”，通常是离散的、可复现的，并且可以通过修改特定的代码行来修复。
+ AI 应用，其行为本质上是非确定性和概率性的。表现出统计性的、上下文相关的故障模式和不可预测的涌现行为，这意味着对于相同的输入，它们可能会产生不同的输出。



传统的 QA 流程，专为可预测的、基于规则的系统而设计，已无法充分应对这些由数据驱动的、自适应系统的挑战。即使在发布阶段进行了充分的测试，也无法保障上线后，面对各种输出稳定性问题。因此，评估不再仅仅是部署前的一个阶段，而是由可观测性平台、持续监控、自动化评估与治理即服务，所构成的评估工程。



## 二、不确定性的根因
AI 应用的非确定性源于其核心技术架构和训练方法。与传统软件不同，AI 应用本质上是概率性系统。它们的核心功能是基于从海量数据中学到的统计模式，来预测序列中的下一个词，而非真正意义上的理解 。这种固有的随机性既是其创造力的源泉，也是其不可靠性的根源，并导致了“幻觉”现象，即模型生成听起来合理但实际上不正确或无意义的输出 。 



产生幻觉和不确定性的根本原因错综复杂：

+ **数据导致的缺陷：** 模型的知识完全受限于其训练数据。如果数据不完整、包含事实错误或反映了社会偏见，模型将会继承并放大这些缺陷 。它无法提供训练数据之外的信息，例如未来的事件或未曾接触过的私有数据 。 
+ **架构与建模的产物：** Transformer 架构及其训练过程本身就会引入不确定性。预测下一个词元的核心任务鼓励模型在信息不足时进行“猜测”。对训练数据的过拟合可能导致模型死记硬背而非泛化，同时，注意力机制的错误可能使其忽略提示中的关键部分 。 
+ **错位与不确定性：** 模型可能拥有正确的知识，但由于与用户的具体指令不完全对齐，导致未能正确应用这些知识 。虽然幻觉通常与模型的不确定性有关，但研究表明，模型也可能在具有高置信度的情况下产生幻觉，这使得此类错误尤其隐蔽且难以检测 。 



## 三、用魔法打败魔法
传统的自动化评估指标，如用于机器翻译的 BLEU（Bilingual Evaluation Understudy） 和用于文本摘要的 ROUGE（Recall-Oriented Understudy for Gisting Evaluation），其核心是基于词汇或短语的重叠度来计算分数。这种方法对于评估需要捕捉语义、风格、语气和创造力等细微差别的现代生成式 AI 模型来说，存在根本性的不足。一个模型生成的文本可能在措辞上与参考答案完全不同，但在语义上却更准确、更具洞察力。传统指标无法识别这种情况，甚至可能给予低分。



另一方面，虽然人工评估被视为评估质量的“黄金标准”，但其高昂的成本、漫长的周期和固有的主观性，使其难以适应 AI 技术快速迭代的开发节奏。这种评估能力的滞后，形成了一个严重的瓶颈，常常导致有潜力的 AI 项目陷入试点困境，无法有效验证和改进。



因此，“用魔法打败魔法”成为评估工程中的新范式，即 LLM-as-a-Judge 自动化评估工具。它利用一个功能强大的大型语言模型（通常是前沿模型）来扮演裁判的角色，对另一个 AI 模型（或应用）的输出进行评分、排序或选择。这种方法巧妙地结合了自动化评估的可扩展性和人工评估的细致性。



## 四、自动化评估工具的开源实践
在 RL／RLHF 场景中，奖励模型（Reward Model, RM）已经成为一种主流的自动化评估工具，以及出现了专门评估奖励模型的基准，如海外的 RewardBench[1] 和国内高校联合发布的 RM Bench[2]，用来测不同 RM 的效果、比较谁能更好地预测人类偏好。下方将介绍 ModelScope 近期开源的奖励模型——RM-Gallery，项目地址：

[https://github.com/modelscope/RM-Gallery/](https://github.com/modelscope/RM-Gallery/)



RM-Gallery 是一个集奖励模型训练、构建与应用于一体的一站式平台，支持任务级与原子级奖励模型的高吞吐、容错实现，助力奖励模型全流程落地。

![](https://img.alicdn.com/imgextra/i1/O1CN01LGPXPV1vYmldxTLbn_!!6000000006185-2-tps-2232-1126.png)



RM-Gallery 提供基于 RL 的推理奖励模型训练框架，兼容主流框架（如 Verl），并提供集成 RM-Gallery 的示例。在 RM Bench 上，经过80步训练，准确率由基线模型（Qwen2.5-14B）的约55.8%提升至约62.5%。

![](https://img.alicdn.com/imgextra/i4/O1CN01Ajs2ji1QC74YP7dAz_!!6000000001939-54-tps-1062-762.apng)  


RM-Gallery 的几个关键特性包括：

+ 支持任务级别和更细粒度的原子级奖励模型。 
+ 提供标准化接口、丰富内置模型库（例如数学正确性、代码质量、对齐、安全等）供直接使用或者定制。 
+ 支持训练流程（使用偏好数据、对比损失、RL 机制等）来提升奖励模型性能。
+ 支持将这些奖励模型用于多个应用场景：比如“Best-of-N 选择”“数据修正”“后训练 / RLHF”场景。



所以，从功能来看，它是将奖励模型——即用于衡量大模型输出好坏、优先级、偏好一致性等，打造成一个可训练、可复用、可部署的用于评估工程的基础设施平台。



当然，构建完整的评估工程，仅是一个奖励模型是不够的，还需要持续采集业务数据，包括用户对话、反馈、调用日志等，从而进一步优化数据集，训练小尺寸模型、甚至教师模型，形成数据的大小飞轮。有关 AI 评估更详细的内容，可下载《AI 原生应用架构白皮书》，阅读第9章 AI 评估。



白皮书下载地址：[https://developer.aliyun.com/ebook/8479](https://developer.aliyun.com/ebook/8479)



[1][https://allenai.org/blog/rewardbench-the-first-benchmark-leaderboard-for-reward-models-used-in-rlhf-1d4d7d04a90b](https://allenai.org/blog/rewardbench-the-first-benchmark-leaderboard-for-reward-models-used-in-rlhf-1d4d7d04a90b?utm_source=chatgpt.com)

[2][https://arxiv.org/html/2410.16184v1](https://arxiv.org/html/2410.16184v1)


